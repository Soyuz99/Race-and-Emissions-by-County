{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview:\n",
        "1. This code is meant to obtain a data set that compares the emissions per county and demographics per county.\n",
        "2. The 2 data sets used were both from the United States government with the demographics by county being obatained from the United States census and the Environmental Protection Agency. the census data is from the 2020 census while the emissions data is from 2021.\n",
        "\n"
      ],
      "metadata": {
        "id": "KxyrPKNXRlwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Get Started\n",
        "### Pandas:\n",
        "Pandas is a package that can be imported to google colab and can assist with reading .csv files.\n",
        "\n",
        "In order to import pandas you need to use `import pandas as pd`\n",
        "\n"
      ],
      "metadata": {
        "id": "EeIBzmaxSBD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I-JLUHVGWvAn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading the File\n",
        "\n",
        "You need to ensure that the .csv files are located in the content folder in order to proceed. You can acess this by clicking the file icon and clicking through util you find a file labeled \"Content.\" Once you find this you can right click and upload the files to the folder.\n",
        "\n",
        "Once this is complete you can now utilize pandas to read the file, in order to do this use `df = pd.read_csv(\"air_emissions.csv\")` and `df2 = pd.read_csv(\"county_demographics.csv\")`. This will allow you to assign a variable and work with the file."
      ],
      "metadata": {
        "id": "QSDSbhjFSoMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A0pW99ifaZXt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "fa61d839-9e67-45e1-c565-19b773da7431"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-81bbedde1239>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"air_emissions.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'air_emissions.csv'"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"air_emissions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"county_demographics.csv\")"
      ],
      "metadata": {
        "id": "qWR6ABC2v-wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exlporing the Datasets\n",
        "\n",
        "In order to perform `.join` you need to esure the column names are the exact same. You should also ensure there are no other complications by \"looking\" at your data. You can use `df.shape`/`df2.shape` to get the dimensions of the dataset. Some other helpful ones are: `df.columns`/`df2.columns` to view the names of the columns to see if they are the same, `df.head()`/`df2.head()` in order to see the first five rows, which can be extremely helpful."
      ],
      "metadata": {
        "id": "u7amV-iiTuNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNpt8QoPcbDC"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exicI1LWcjQ4"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZVQ6Dvgcraz"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvxPLG4ecxIo"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obxCJEbGdC3S"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHdTv6lzLZUz"
      },
      "outputs": [],
      "source": [
        "df2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1iyfHNvLj6r"
      },
      "outputs": [],
      "source": [
        "df2.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onGYCJNDLtQi"
      },
      "outputs": [],
      "source": [
        "df2.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieFFzJpAQc9E"
      },
      "outputs": [],
      "source": [
        "df2.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Culling the Data and Selecting the Data Needed\n",
        "\n",
        "In the original files, there is data that is not used and unecessary for this subset of data, so you need to select the columns that are desired. You can do this using `sdf = df[[\n",
        "    'Total reported direct emissions from Local Distribution Companies',\n",
        "    'Nitrous Oxide (N2O) emissions ',\n",
        "    'Methane (CH4) emissions ',\n",
        "    'CO2 emissions (non-biogenic) ',\n",
        "    'Does the facility employ continuous emissions monitoring? ',\n",
        "    'Reported County',\n",
        "    'Reported State'\n",
        "]]` to select the columns needed. These specific columns were chosen becuase they include the emissions per county data and it includes multiple emissions."
      ],
      "metadata": {
        "id": "6uJkTA33VA54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = df[[\n",
        "    'Total reported direct emissions from Local Distribution Companies',\n",
        "    'Nitrous Oxide (N2O) emissions ',\n",
        "    'Methane (CH4) emissions ',\n",
        "    'CO2 emissions (non-biogenic) ',\n",
        "    'Does the facility employ continuous emissions monitoring? ',\n",
        "    'Reported County',\n",
        "    'Reported State'\n",
        "]]"
      ],
      "metadata": {
        "id": "9lSJVQ4kyqZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, you need to select the columns desired for the second data set using `sdf2 = df2[[\n",
        "    'County',\n",
        "    'State',\n",
        "    'Ethnicities.American Indian and Alaska Native Alone',\n",
        "'Ethnicities.Asian Alone',\n",
        "'Ethnicities.Black Alone',\n",
        "'Ethnicities.Hispanic or Latino',\n",
        "'Ethnicities.Native Hawaiian and Other Pacific Islander Alone',\n",
        "'Ethnicities.Two or More Races',\n",
        "'Ethnicities.White Alone',\n",
        "'Ethnicities.White Alone\\t not Hispanic or Latino',\n",
        "]]`. These specific columns were chosen because they include the demographics by county, the county column is important as this is the column the `.join` will utilize."
      ],
      "metadata": {
        "id": "ln6Uv5tTVjXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf2 = df2[[\n",
        "    'County',\n",
        "    'State',\n",
        "    'Ethnicities.American Indian and Alaska Native Alone',\n",
        "'Ethnicities.Asian Alone',\n",
        "'Ethnicities.Black Alone',\n",
        "'Ethnicities.Hispanic or Latino',\n",
        "'Ethnicities.Native Hawaiian and Other Pacific Islander Alone',\n",
        "'Ethnicities.Two or More Races',\n",
        "'Ethnicities.White Alone',\n",
        "'Ethnicities.White Alone\\t not Hispanic or Latino',\n",
        "]]"
      ],
      "metadata": {
        "id": "6GtJquhz09A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining the Datasets\n",
        "\n",
        "As said above, the column name that is used for `.join`. This being said, the columns do not match up for this data, so we need to edit the column titles. You can do this by using `sdf2['County'] = sdf2['County'].str.upper()` which will change the column name to \"COUNTY\". There was also a need to remove \"REPORTED\" in order to make the two columns exactly the the same. To add this you need to use `sdf['County'] = sdf['Reported County']`. This will change the title of the column to \"COUNTY\". Now we have an overlapping column."
      ],
      "metadata": {
        "id": "yHqYW6NwV6-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf2['County'] = sdf2['County'].str.upper()"
      ],
      "metadata": {
        "id": "adzQYKOa15V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf2.head()"
      ],
      "metadata": {
        "id": "fpO9gvOU2UKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf['County'] = sdf['Reported County']"
      ],
      "metadata": {
        "id": "pHP5qXQa2yff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.head()"
      ],
      "metadata": {
        "id": "tKi09Z7Q5UQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to `.join` in this case the index needed to be chnaged to match. To do this you need to use `sdf2.set_index('County', inplace=True)`, this will chnage the index, you will also need to do it with the other data set by using `sdf.set_index('County', inplace=True)`. Now that these have been changed the index now overlaps. The `.set_index()` cannot be ran within the same line, jsut ensure that they are in separte lines or it will not work. Once that is done you can now use `fdf = sdf.join(sdf2,lsuffix='_Emissions', how= 'inner')` to join the two data sets. In this scenario we used an \"inner\" method of joining them, which will only match columns and rows on the basis that their column titles match and the information within the column titles."
      ],
      "metadata": {
        "id": "lXnYQ8rlOgHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdf2.set_index('County', inplace=True)"
      ],
      "metadata": {
        "id": "K4PHtbFfMiFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.set_index('County', inplace=True)\n",
        "\n",
        "fdf = sdf.join(sdf2,lsuffix='_Emissions', how= 'inner')"
      ],
      "metadata": {
        "id": "GV-TITNN3Zdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exporting the Data\n",
        "\n",
        "Ensure that the data looks correct by using `fdf.heaad()`. If it does look correct you can now start exporting your data. to do this you can use `fdf.to_csv('Race_by_County_and_Emissions_by_County.csv')`. After you run this you should see the .csv file in the content folder. From there you can right click the file and donwload it."
      ],
      "metadata": {
        "id": "aqXj0ZRUP2Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdf.head()"
      ],
      "metadata": {
        "id": "s0QptL7B7rmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdf.to_csv('Race_by_County_and_Emissions_by_County.csv')"
      ],
      "metadata": {
        "id": "0q8qpnOKadYQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
